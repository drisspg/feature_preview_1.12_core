{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4978a4-459e-496d-b83f-4b668329b619",
   "metadata": {},
   "source": [
    "# Lazy Tensor\n",
    "- High level overview https://pytorch.org/blog/understanding-lazytensor-system-performance-with-pytorch-xla-on-cloud-tpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b1597-ccee-4ecb-a1d1-0dd5fbfb1bc8",
   "metadata": {},
   "source": [
    "### This collab is HEAVILY inspired from this \n",
    "https://github.com/pytorch/pytorch/blob/master/torch/csrc/lazy/tutorial.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4d90df-dceb-48ca-9988-54c914b7972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Conditional logic hard for hardware vendors to account for\n",
    "\n",
    "def add_two_maybe(t: torch.Tensor, maybe: torch.Tensor):\n",
    "    if maybe:\n",
    "        return t + 2\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ca3f53-3d7f-465f-8b32-c30c9227c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/cvs2rnpx60qc3b4_x72xvxlr0000gn/T/ipykernel_29711/1870170415.py:6: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if maybe:\n"
     ]
    }
   ],
   "source": [
    "# Lets use our existing tracing system\n",
    "t = torch.ones(1)\n",
    "maybe_false = torch.BoolTensor([0])\n",
    "good_inputs = (t, maybe_false)\n",
    "jit = torch.jit.trace(add_two_maybe, good_inputs)\n",
    "# let's check that the results match with eager\n",
    "assert jit(*good_inputs) == add_two_maybe(*good_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86b3b9c-b1f3-4cd5-8b72-3c343524273f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m maybe_true \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mBoolTensor([\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m jit(t, maybe_true) \u001b[38;5;241m==\u001b[39m add_two_maybe(t, maybe_true)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maybe_true = torch.BoolTensor([1])\n",
    "assert jit(t, maybe_true) == add_two_maybe(t, maybe_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce6e9a0-d2cb-44a8-a1bc-ea5e5479559d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%t : Tensor,\n",
      "      %maybe : Tensor):\n",
      "  return (%t)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print jit graph:\n",
    "print(torch.jit.last_executed_optimized_graph())\n",
    "\n",
    "# No if statemnt in graph just return the else path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14b6f01-aa9c-4bbd-a451-a2863d906d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy tensors to the rescue\n",
    "# lazy device remebers aten ops called with what inputs as opposed to \n",
    "# eargly executing them\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# Lazy imports\n",
    "import torch._lazy\n",
    "import torch._lazy.ts_backend\n",
    "import torch._lazy.metrics\n",
    "torch._lazy.ts_backend.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c0288d-2b72-42d7-b062-3846b8bdaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A virtual \"lazy\" device \n",
    "dev = \"lazy\"\n",
    "t_lazy = torch.ones(1).to(dev)\n",
    "maybe_false_lazy = torch.BoolTensor([0]).to(dev)\n",
    "lazy_result = add_two_maybe(t_lazy, maybe_false_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec52f61-53a9-4790-9dbd-de7da84dfa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='lazy:0')\n"
     ]
    }
   ],
   "source": [
    "# Printing triggers execution of the op\n",
    "print(lazy_result)\n",
    "assert lazy_result.cpu() == add_two_maybe(t, maybe_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb07f898-a394-46ef-b3fb-3b5722bbd3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the case that Jit couldn't handle:\n",
    "maybe_true_lazy = torch.BoolTensor([1]).to(dev)\n",
    "lazy_result = add_two_maybe(t_lazy, maybe_true_lazy)\n",
    "assert lazy_result.cpu() == add_two_maybe(t, maybe_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df87ff-cec5-46b5-9a27-82a11c3d8c7b",
   "metadata": {},
   "source": [
    "## Downsides\n",
    "- Overhead for backends to translate aten ops to lower level for hardware\n",
    "- Depends on model amount of dynamicism. The less dynamic the greater the reward for generating a trace and compiling but if super dynamic then there will be non trivial amount of time re-tracing and re-compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f374ab40-5d0b-420a-a856-0082fa4f6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c6e4529-f9b2-4b01-8e39-ba066ac144aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # Forward traced\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Backward traced\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Regular Training loop execpt for this func call which\n",
    "        # instructs Lazy Tensor to break up the current trace \n",
    "        # and start executing it asynchronously.\n",
    "        torch._lazy.mark_step()\n",
    "        \n",
    "        # if batch_idx %2 ==0:\n",
    "        #    torch._lazy.mark_step() \n",
    "        # can do this but don't need to capture\n",
    "        # multiple forward backward passes in one trace\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            #  Print is a blocking call that will cause execution to pause\n",
    "            # so that the loss can be computed and printed\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddac5d7-a769-42a3-a23f-2cab37a2843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 64\n",
    "device = 'lazy'\n",
    "epochs = 14\n",
    "log_interval = 10\n",
    "lr = 1\n",
    "gamma = 0.7\n",
    "train_kwargs = {'batch_size': bsz}\n",
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "\n",
    "# my computer is fast but not that fast sowe shorten data\n",
    "dataset1.data = dataset1.data[:6000,:,:]\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "# Move the model to the lazy device\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df2f2e-d84d-4d5c-a1a9-657010c47a1f",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "- Not full op coverage covers the top 100 (However the current trace will break, it will concretize all the inputs to the current un-supported op and run on a suppported device and then shifts everything back to lazy device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c79fe005-3d4b-41b1-b3c8-b980424f1dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.153067\n",
      "Train Epoch: 1 [640/6000 (11%)]\tLoss: 0.038211\n",
      "Train Epoch: 1 [1280/6000 (21%)]\tLoss: 0.181790\n",
      "Train Epoch: 1 [1920/6000 (32%)]\tLoss: 0.166017\n",
      "Train Epoch: 1 [2560/6000 (43%)]\tLoss: 0.064924\n",
      "Train Epoch: 1 [3200/6000 (53%)]\tLoss: 0.049524\n",
      "Train Epoch: 1 [3840/6000 (64%)]\tLoss: 0.035806\n",
      "Train Epoch: 1 [4480/6000 (74%)]\tLoss: 0.180171\n",
      "Train Epoch: 1 [5120/6000 (85%)]\tLoss: 0.245858\n",
      "Train Epoch: 1 [5760/6000 (96%)]\tLoss: 0.113180\n",
      "['CachedCompile',\n",
      " 'CreateLtcTensor',\n",
      " 'DestroyLtcTensor',\n",
      " 'DeviceDataCacheMiss',\n",
      " 'MarkStep',\n",
      " 'UncachedCompile',\n",
      " 'aten::_local_scalar_dense',\n",
      " 'lazy::_copy_from',\n",
      " 'lazy::_log_softmax',\n",
      " 'lazy::_log_softmax_backward_data',\n",
      " 'lazy::_to_copy',\n",
      " 'lazy::add',\n",
      " 'lazy::addcmul',\n",
      " 'lazy::addmm',\n",
      " 'lazy::convolution',\n",
      " 'lazy::convolution_backward',\n",
      " 'lazy::div',\n",
      " 'lazy::fill_',\n",
      " 'lazy::max_pool2d_with_indices',\n",
      " 'lazy::max_pool2d_with_indices_backward',\n",
      " 'lazy::mm',\n",
      " 'lazy::mul',\n",
      " 'lazy::native_dropout',\n",
      " 'lazy::native_dropout_backward',\n",
      " 'lazy::nll_loss_backward',\n",
      " 'lazy::nll_loss_forward',\n",
      " 'lazy::relu',\n",
      " 'lazy::sqrt',\n",
      " 'lazy::sum',\n",
      " 'lazy::t',\n",
      " 'lazy::threshold_backward',\n",
      " 'lazy::view',\n",
      " 'lazy::zero_functional']\n"
     ]
    }
   ],
   "source": [
    "torch._lazy.metrics.reset()\n",
    "train(log_interval, model, device, train_loader, optimizer, 1)\n",
    "\n",
    "# Any op with aten prefix is not supported with on lazy tensor device\n",
    "from pprint import pprint\n",
    "pprint(torch._lazy.metrics.counter_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea50fa9d-0990-49e2-baff-a10c31315e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while mark_step executes op asynchrosnly wait_device_ops() is a blocking op\n",
    "torch._lazy.wait_device_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2b371-936b-4375-a71b-a25662f3d9f7",
   "metadata": {},
   "source": [
    "### Blog on implemntation with torch xla\n",
    "https://pytorch.org/blog/understanding-lazytensor-system-performance-with-pytorch-xla-on-cloud-tpu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f312234-ec02-4798-850a-a4199ffb3597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
