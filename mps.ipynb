{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5994e7-e53a-488a-ae2a-f3f615126933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb8343-4c6f-4716-9200-8bc19412e6de",
   "metadata": {},
   "source": [
    "# MPS backend\n",
    "- New backend is now available named MPS (Metal Perfomance Shaders)\n",
    "- Can be used as drop in replacment for the 'cuda' device\n",
    "- *Caveats* not all the ops are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7ae416-2604-4767-92ad-81852b642854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1a046f5-563b-4980-8df7-d33e13aea5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "import torchvision.models as models\n",
    "\n",
    "# Small helper which will be used for profiling inference\n",
    "def run_in_inf_mode(model, tensor):\n",
    "    with torch.inference_mode():\n",
    "        model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a71a8f9-b84e-43bd-9d6b-a50378c6df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility allow for benchmarking pytorch code\n",
    "# https://pytorch.org/tutorials/recipes/recipes/benchmark.html\n",
    "\n",
    "# Other Comparisons\n",
    "# https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html\n",
    "\n",
    "import torch.utils.benchmark as benchmark\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12f394aa-bd67-4e91-8853-084fb7cb9f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************Vgg*************************************************\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x12fcf2b30>\n",
      "Vgg on CPU\n",
      "  Median: 3.96 s\n",
      "  3 measurements, 1 runs per measurement, 10 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x29085ba00>\n",
      "Vgg on MPS\n",
      "  Median: 510.24 ms\n",
      "  IQR:    3.37 ms (509.61 to 512.98)\n",
      "  18 measurements, 1 runs per measurement, 10 threads\n"
     ]
    }
   ],
   "source": [
    "# Profile VGG\n",
    "# Import a network we know works with mps backend\n",
    "vgg_weights = models.VGG16_BN_Weights.DEFAULT\n",
    "vgg_preprocessor = vgg_weights.transforms()\n",
    "\n",
    "vgg = models.vgg.vgg16_bn(weights=vgg_weights)\n",
    "vgg_cpu = copy.deepcopy(vgg).eval()\n",
    "vgg_mps = vgg.to(device).eval()\n",
    "\n",
    "# Input for benchmarking:\n",
    "x_cpu = vgg_preprocessor(torch.randint(0, 256, size=(64, 3, 224, 224)))\n",
    "x_mps = x_cpu.clone().to(device)\n",
    "\n",
    "\n",
    "print(\"Vgg\".center(100,\"*\"))\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='run_in_inf_mode(vgg_cpu, x_cpu)',\n",
    "    globals={'run_in_inf_mode':run_in_inf_mode,'vgg_cpu':vgg_cpu, 'x_cpu': x_cpu},\n",
    "    label='Vgg on CPU',\n",
    "    num_threads=torch.get_num_threads())\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='run_in_inf_mode(vgg_mps, x_mps)',\n",
    "    globals={'run_in_inf_mode':run_in_inf_mode,'vgg_mps':vgg_mps, 'x_mps':x_mps},\n",
    "    label='Vgg on MPS',\n",
    "    num_threads=torch.get_num_threads())\n",
    "\n",
    "m0 = t0.blocked_autorange(min_run_time=10)\n",
    "m1 = t1.blocked_autorange(min_run_time=10)\n",
    "\n",
    "print(m0)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4372b41c-c1fe-4219-bc7f-d4e5bc90e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************EfficientNet********************************************\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x1483da200>\n",
      "EfficientNet on CPU\n",
      "  Median: 4.00 s\n",
      "  3 measurements, 1 runs per measurement, 10 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x156d602e0>\n",
      "EfficientNet on MPS\n",
      "  Median: 291.97 ms\n",
      "  IQR:    3.31 ms (291.36 to 294.67)\n",
      "  35 measurements, 1 runs per measurement, 10 threads\n"
     ]
    }
   ],
   "source": [
    "# Profile Efficient Net\n",
    "# Import a network we know works with mps backend\n",
    "eff_weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "eff_preprocessor = eff_weights.transforms()\n",
    "\n",
    "efficientnet_b0 = models.efficientnet_b0(weights=eff_weights)\n",
    "effic_cpu = copy.deepcopy(efficientnet_b0).eval()\n",
    "effic_mps = efficientnet_b0.to(device).eval()\n",
    "\n",
    "# Input for benchmarking:\n",
    "x_cpu = eff_preprocessor(torch.randint(0, 256, size=(64, 3, 224, 224)))\n",
    "x_mps = x_cpu.clone().to(device)\n",
    "\n",
    "\n",
    "print(\"EfficientNet\".center(100,\"*\"))\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='run_in_inf_mode(effic_cpu, x_cpu)',\n",
    "    globals={'run_in_inf_mode':run_in_inf_mode,'effic_cpu':effic_cpu, 'x_cpu': x_cpu},\n",
    "    label='EfficientNet on CPU',\n",
    "    num_threads=torch.get_num_threads())\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='run_in_inf_mode(effic_mps, x_mps)',\n",
    "    globals={'run_in_inf_mode':run_in_inf_mode,'effic_mps':effic_mps, 'x_mps':x_mps},\n",
    "    label='EfficientNet on MPS',\n",
    "    num_threads=torch.get_num_threads())\n",
    "\n",
    "m0 = t0.blocked_autorange(min_run_time=10)\n",
    "m1 = t1.blocked_autorange(min_run_time=10)\n",
    "\n",
    "print(m0)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1fc4bed-1c9d-425c-8987-d37678685a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd works for these ops:\n",
    "vgg_weights = models.VGG16_BN_Weights.DEFAULT\n",
    "vgg_preprocessor = vgg_weights.transforms()\n",
    "\n",
    "vgg = models.vgg.vgg16_bn(weights=vgg_weights)\n",
    "vgg_cpu = copy.deepcopy(vgg)\n",
    "vgg_mps = vgg.to(device)\n",
    "\n",
    "# Input for benchmarking:\n",
    "x_cpu = vgg_preprocessor(torch.randint(0, 256, size=(64, 3, 224, 224)))\n",
    "x_mps = x_cpu.clone().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2b037c2-6728-45de-8ad9-7ce6ccd86ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.2 ms, sys: 2.44 s, total: 2.48 s\n",
      "Wall time: 3.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg_mps(x_mps).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c3a8385-5fe1-43fd-8c2e-ba840d2afa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.9 s, sys: 7.61 s, total: 48.5 s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vgg_cpu(x_cpu).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cdc4ae5-07fe-4f4c-aa47-ccfe93166672",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::equal' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Not all operators are avialble though:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# See this issue for tracking: https://github.com/pytorch/pytorch/issues/77764\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::equal' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# Not all operators are avialble though:\n",
    "# See this issue for tracking: https://github.com/pytorch/pytorch/issues/77764\n",
    "torch.equal(torch.randn(1,3,device=device),torch.randn(1,3,device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec0a93c5-e1f9-4264-a9e2-7eac1c267532",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_slow_conv2d_forward' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m res_mps \u001b[38;5;241m=\u001b[39m res50\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Input for benchmarking:\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mres50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m x_mps \u001b[38;5;241m=\u001b[39m x_cpu\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet50\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mcenter(\u001b[38;5;241m100\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/feature_preview/lib/python3.10/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/feature_preview/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/feature_preview/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/feature_preview/lib/python3.10/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/feature_preview/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/feature_preview/lib/python3.10/site-packages/torch/nn/modules/conv.py:455\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    453\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    454\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_slow_conv2d_forward' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# Profile Resnet50 Breaks on MPS\n",
    "res50_weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "preprocessor = res50_weights.transforms()\n",
    "res50 = models.resnet50(weights=weights)\n",
    "res_cpu = copy.deepcopy(res50).eval()\n",
    "res_mps = res50.to(device).eval()\n",
    "\n",
    "# Input for benchmarking:\n",
    "x_cpu = res50(torch.randint(0, 256, size=(64, 3, 224, 224)))\n",
    "x_mps = x_cpu.clone().to(device)\n",
    "\n",
    "print(\"ResNet50\".center(100,\"*\"))\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='run_in_inf_mode(effic_cpu, x_cpu)',\n",
    "    globals={'run_in_inf_mode':run_in_inf_mode,'res_cpu':res_cpu, 'x_cpu': x_cpu},\n",
    "    label='ResNet50 on CPU',\n",
    "    num_threads=torch.get_num_threads())\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='run_in_inf_mode(effic_mps, x_mps)',\n",
    "    globals={'run_in_inf_mode':run_in_inf_mode,'res_mps':res_mps, 'x_mps':x_mps},\n",
    "    label='ResNet50 on MPS',\n",
    "    num_threads=torch.get_num_threads())\n",
    "\n",
    "m0 = t0.blocked_autorange(min_run_time=10)\n",
    "m1 = t1.blocked_autorange(min_run_time=10)\n",
    "\n",
    "print(m0)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f5347-30c9-49fd-8f5e-009ccc527e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
