{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063d7353-5ea7-49ce-a0b3-2cb73c0de523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.stateless import functional_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78207e-098c-4d0e-83e4-c14bf25aacee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Torch NN Stateless \n",
    "- Turn stateful nn modules into to stateless functional form\n",
    "- Interesting discussion can be found here: https://github.com/pytorch/pytorch/issues/49171\n",
    "- https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/stateless.py\n",
    "Code interesteing read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abc2eeb-587f-4d5a-a77d-7d697aa95db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.foo = torch.zeros(1)\n",
    "    def forward(self, a):\n",
    "        self.foo =  self.foo + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa7191e-af4c-4eb0-b609-2797f04d462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([0.])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "a = {'foo': torch.zeros(())}\n",
    "mod = Foo()  # does self.foo = self.foo + 1\n",
    "print(mod.foo)  # tensor(0.)\n",
    "functional_call(mod, a, torch.ones(()))\n",
    "print(mod.foo)  # tensor(0.)\n",
    "print(a['foo'])  # tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba7e428-d30e-417f-986f-1a8ef837d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n",
      "torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "# Create module and call it functional\n",
    "lin_mod = nn.Linear(10,15)\n",
    "param_dict = dict(lin_mod.named_parameters())\n",
    "output = functional_call(lin_mod, param_dict, torch.randn(1,10))\n",
    "# Should be same as calling lin_mod 1x15\n",
    "print(output.shape)\n",
    "\n",
    "# what if we wanted to reuse module but diffent weights\n",
    "new_params = {\"weight\": torch.randn(20,10), \"bias\": torch.randn(20)}\n",
    "output = functional_call(lin_mod, new_params, torch.randn(1,10))\n",
    "# Same computation diffent tensors should be 1x20\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17227e-883b-4ae1-a84b-cb7b3dd4e265",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/master/generated/torch.nn.utils.stateless.functional_call.html#torch.nn.utils.stateless.functional_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf4f827-7a2f-4fb4-8b5d-13a6a25bd458",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The stateless API can't be used with Jitted modules",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjitted\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/feature_preview/lib/python3.10/site-packages/torch/nn/utils/stateless.py:134\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# TODO allow kwargs such as unsafe and others for parametrization\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    126\u001b[0m         torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing()\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m         )\n\u001b[1;32m    133\u001b[0m ):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe stateless API can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be used with Jitted modules\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The stateless API can't be used with Jitted modules"
     ]
    }
   ],
   "source": [
    "jitted = torch.jit.trace(lin_mod, example_inputs=torch.randn(20,10))\n",
    "functional_call(jitted,param_dict, torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30465f66-97e9-474f-90a9-6212e53fff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward mode AD\n",
    "#Currenlty can not store dual tensors as nn.paramaters so this provides a work around\n",
    "\n",
    "# We need a fresh module because the functional call requires the\n",
    "# the model to have parameters registered.\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "\n",
    "model = nn.Linear(5, 5)\n",
    "input = torch.randn(16, 5)\n",
    "\n",
    "params = {name: p for name, p in model.named_parameters()}\n",
    "tangents = {name: torch.rand_like(p) for name, p in params.items()}\n",
    "\n",
    "# Old way of doing things\n",
    "with fwAD.dual_level():\n",
    "    for name, p in params.items():\n",
    "        delattr(model, name)\n",
    "        setattr(model, name, fwAD.make_dual(p, tangents[name]))\n",
    "\n",
    "    out = model(input)\n",
    "    jvp = fwAD.unpack_dual(out).tangent\n",
    "\n",
    "# Use stateless to replace paramaters with dual paramaters\n",
    "\n",
    "dual_params = {}\n",
    "with fwAD.dual_level():\n",
    "    for name, p in params.items():\n",
    "        # Using the same ``tangents`` from the above section\n",
    "        dual_params[name] = fwAD.make_dual(p, tangents[name])\n",
    "    out = functional_call(model, dual_params, input)\n",
    "    jvp2 = fwAD.unpack_dual(out).tangent\n",
    "\n",
    "# Check our results\n",
    "assert torch.allclose(jvp, jvp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d113f-f1fd-47c6-819d-1c87a727ea08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
