{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b50dc7-23bd-4953-8bca-8ca2abd9d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils._pytree import tree_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96434495-c6d8-40b8-b450-93def91e23c4",
   "metadata": {},
   "source": [
    "# Tensor Subclass\n",
    "- Huge In terms of scope and alot to breakdown\n",
    "- Read this for fuller picture: https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557\n",
    "- Adds extensibility from the python layer instead of having to drop further down into c++\n",
    "- Examples of subclasses: https://github.com/albanD/subclass_zoo\n",
    "- Extend the dispatcher with no need to drop down to c++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7071ab89-d512-44dd-ab77-fa78bc677ee5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseTensor(torch.Tensor):\n",
    "    # See https://github.com/pytorch/pytorch/pull/73727 ; this is necessary\n",
    "    # to ensure that super().__new__ can cooperate with each other\n",
    "    @staticmethod\n",
    "    def __new__(cls, elem, *, requires_grad=None):\n",
    "        if requires_grad is None:\n",
    "            return super().__new__(cls, elem)\n",
    "        else:\n",
    "            return cls._make_subclass(cls, elem, requires_grad)\n",
    "\n",
    "    # To ensure constructors can cooperate with one another, must accept and\n",
    "    # ignore element tensor (TODO: is this right???)\n",
    "    def __init__(self, elem):\n",
    "        super().__init__()\n",
    "\n",
    "    # If __torch_dispatch__ is defined (which it will be for all our examples)\n",
    "    # the default torch function implementation (which preserves subclasses)\n",
    "    # typically must be disabled\n",
    "    __torch_function__ = torch._C._disabled_torch_function_impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905eb405-d563-4b5b-853e-54a50e7426c5",
   "metadata": {},
   "source": [
    "### Scaffolding \n",
    "- https://github.com/albanD/subclass_zoo/blob/main/trivial_tensors.py This is the tensor to use for building a new subclass tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f361e218-5c40-4adb-9c29-4931acd9fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import FunctionType\n",
    "\n",
    "import torch\n",
    "# from base_tensor import BaseTensor\n",
    "from torch import Tensor\n",
    "from torch.fx import Graph, GraphModule, Tracer\n",
    "from torch.fx.passes.shape_prop import _extract_tensor_metadata\n",
    "from torch.testing._internal.common_utils import run_tests, TestCase\n",
    "from torch.utils._pytree import tree_map\n",
    "\n",
    "\n",
    "class TrivialTensorViaInheritance(BaseTensor):\n",
    "    \"\"\"\n",
    "    TrivialTensorViaInheritance extends tensor behavior using inheritance (\"is\n",
    "    a\").  These implementations are very straightforward and we recommend\n",
    "    using them if it works for your use case.  To get the base behavior,\n",
    "    you use standard object-oriented idiom of super().\n",
    "    Benefits and downsides of this representation:\n",
    "        + Efficient representation (only one tensor).\n",
    "        + Do not have to worry about synchronizing metadata between the inner\n",
    "          and outer tensor.\n",
    "        = Requires multiple inheritance to do composition.  This *does*\n",
    "          work, but it is a bit mind-bending, you have to deal with the\n",
    "          diamond inheritance problem, and traditionally you only get a fixed\n",
    "          set of composition (rather than dynamic, as in functorch) unless\n",
    "          you're willing to generate classes on the fly.\n",
    "        - Doesn't work if you need to run internal PyTorch subsystems\n",
    "          (e.g., autograd) multiple times.\n",
    "        - Doesn't work if the internal tensor has a different shape\n",
    "          than the outer tensor.\n",
    "        - Doesn't work if you need multiple internal tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "        def wrap(t):\n",
    "            # When could the returned tensor already be our subclass?\n",
    "            # The most common situation is when an input tensor\n",
    "            # is returned as an output tensor, e.g., inplace or out\n",
    "            # implementations.\n",
    "            if isinstance(t, torch.Tensor) and not isinstance(t, cls):\n",
    "                return cls(t)\n",
    "            else:\n",
    "                return t\n",
    "\n",
    "        return tree_map(wrap, super().__torch_dispatch__(func, types, args, kwargs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf313f-b61e-428a-a3a3-39c632c8fd9a",
   "metadata": {},
   "source": [
    "### Example of subclassed tensor that is defined internal and used alot for testing: LoggingTensor\n",
    "- https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/logging_tensor.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "020694b5-bba3-409c-9cc1-e52ae81ac82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$2 = torch._ops.aten.mm.default($0, $1)',\n",
      " '$3 = torch._ops.aten.pow.Tensor_Scalar($2, 2)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.testing._internal.logging_tensor import LoggingTensor, capture_logs, capture_logs_with_logging_tensor_mode\n",
    "from pprint import pprint\n",
    "x = torch.randn(2,10) \n",
    "y = torch.randn(10,2)\n",
    "\n",
    "with capture_logs_with_logging_tensor_mode() as log:\n",
    "    (x@y)**2\n",
    "pprint(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98f156ae-a8bc-4283-9174-5a4d357aa28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$2 = torch._ops.aten.mm.default($0, $1)',\n",
      " '$3 = torch._ops.aten.pow.Tensor_Scalar($2, 3)']\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,10, device=\"mps\") \n",
    "y = torch.randn(10,2, device=\"mps\")\n",
    "\n",
    "with capture_logs_with_logging_tensor_mode() as log:\n",
    "    c=(x@y)**3\n",
    "pprint(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344a435-5b8d-48f9-b22f-c3a5a1589e17",
   "metadata": {},
   "source": [
    "## How does logging tensor do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622b490-b5c1-4397-b803-9828bf45931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@classmethod\n",
    "def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n",
    "    def unwrap(e):\n",
    "        return e.elem if isinstance(e, cls) else e\n",
    "\n",
    "    def wrap(e):\n",
    "        return cls(e) if isinstance(e, torch.Tensor) else e\n",
    "\n",
    "    with cls.context():\n",
    "        rs = tree_map(wrap, func(*tree_map(unwrap, args), **tree_map(unwrap, kwargs)))\n",
    "        # The above line essentially calls the function on the unwraped tesnor and then rewraps\n",
    "        \n",
    "    # added functionality come here which logs the the module and the name of the func as well as args and kwargs and result\n",
    "    logging.getLogger(\"LoggingTensor\").info(f\"{func.__module__}.{func.__name__}\", args, kwargs, rs)\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1bf4ff-6f2b-4747-b160-1eb8a8e00a9f",
   "metadata": {},
   "source": [
    "### Other cool use cases:\n",
    "\n",
    "- Fairseq using this for offloading tensors to SSD: [here](https://github.com/facebookresearch/fairscale/blob/6f18e779a794badba1fc19bb161ed4382fd337f7/fairscale/experimental/nn/ssd_offload.py) \n",
    "\n",
    "- Flop counter that works with forward and backward: [here](https://pastebin.com/AkvAyJBw)\n",
    "\n",
    "- An intern was able to implement a memory profiler in a week with __torch_dispatch__: [here](https://fb.workplace.com/notes/116830607720079/ )\n",
    "\n",
    "- Potentially adding a new device without any c++ code:[here](https://github.com/albanD/subclass_zoo/pull/36/files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66eaf5-89f6-42dc-b0f1-dfd9c786e875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201499ff-11df-4817-8576-3b1b55ada040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c79e5-5966-49a2-abff-eb19b41e553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dd022-c736-4fa8-80b9-cf7651f10d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed57dd3b-cb55-4811-88ee-ceadd7b87775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace big models\n",
    "import torchvision.models as models\n",
    "eff_weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "eff_preprocessor = eff_weights.transforms()\n",
    "\n",
    "efficientnet_b0 = models.efficientnet_b0(weights=eff_weights)\n",
    "effic_cpu = efficientnet_b0.eval()\n",
    "\n",
    "# Input for benchmarking:\n",
    "x_cpu = eff_preprocessor(torch.randint(0, 256, size=(64, 3, 224, 224)))\n",
    "\n",
    "with capture_logs() as logs:\n",
    "    x = LoggingTensor(x_cpu)\n",
    "    log_input(\"x\", x)\n",
    "\n",
    "    efficientnet_b0(x)\n",
    "    \n",
    "logs = [log[:log.find('Parameter containing')] for log in logs]\n",
    "print('\\n'.join(logs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
